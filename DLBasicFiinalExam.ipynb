{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DQOeQW8_FWg",
        "outputId": "aa739211-1676-4a59-85b1-6106be7c58ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch torchvision\n",
        "!pip install Pillow\n",
        "!pip install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6TbKheJ_QXv",
        "outputId": "022128f8-c06e-4790-dc22-78b807e8bfe4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from zipfile import ZipFile\n",
        "\n",
        "# # データの解凍\n",
        "# data_dir = \"/content/drive/MyDrive/dl-2024-final/data\"\n",
        "# with ZipFile(f\"{data_dir}/train.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(data_dir)\n",
        "# with ZipFile(f\"{data_dir}/valid.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(data_dir)"
      ],
      "metadata": {
        "id": "08s6AzJ0SKPK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import time\n",
        "from statistics import mode\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "def process_text(text):\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # 数詞を数字に変換\n",
        "    num_word_to_digit = {\n",
        "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
        "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
        "        'ten': '10'\n",
        "    }\n",
        "    for word, digit in num_word_to_digit.items():\n",
        "        text = text.replace(word, digit)\n",
        "\n",
        "    # 小数点のピリオドを削除\n",
        "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
        "\n",
        "    # 冠詞の削除\n",
        "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
        "\n",
        "    # 短縮形のカンマの追加\n",
        "    contractions = {\n",
        "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
        "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
        "    }\n",
        "    for contraction, correct in contractions.items():\n",
        "        text = text.replace(contraction, correct)\n",
        "\n",
        "    # 句読点をスペースに変換\n",
        "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
        "\n",
        "    # 句読点をスペースに変換\n",
        "    text = re.sub(r'\\s+,', ',', text)\n",
        "\n",
        "    # 連続するスペースを1つに変換\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "class VQADataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, df_path, image_dir, transform=None, tokenizer=None, answer=True):\n",
        "        self.transform = transform  # 画像の前処理\n",
        "        self.image_dir = image_dir  # 画像ファイルのディレクトリ\n",
        "        self.df = pd.read_json(df_path)  # 画像ファイルのパス，question, answerを持つDataFrame\n",
        "        self.answer = answer\n",
        "        self.tokenizer = tokenizer  # Tokenizer for questions\n",
        "\n",
        "        # 質問文に含まれる単語を辞書に追加\n",
        "        self.question2idx = {}\n",
        "        self.idx2question = {}\n",
        "        for question in self.df[\"question\"]:\n",
        "            question = process_text(question)\n",
        "            words = question.split(\" \")\n",
        "            for word in words:\n",
        "                if word not in self.question2idx:\n",
        "                    self.question2idx[word] = len(self.question2idx)\n",
        "        self.idx2question = {v: k for k, v in self.question2idx.items()}  # 逆変換用の辞書(question)\n",
        "\n",
        "        # 回答に含まれる単語を辞書に追加\n",
        "        self.answer2idx = {}\n",
        "        self.idx2answer = {}\n",
        "        if self.answer:\n",
        "            for answers in self.df[\"answers\"]:\n",
        "                for answer in answers:\n",
        "                    word = process_text(answer[\"answer\"])\n",
        "                    if word not in self.answer2idx:\n",
        "                        self.answer2idx[word] = len(self.answer2idx)\n",
        "            self.idx2answer = {v: k for k, v in self.answer2idx.items()}  # 逆変換用の辞書(answer)\n",
        "\n",
        "    def update_dict(self, dataset):\n",
        "        self.question2idx = dataset.question2idx\n",
        "        self.answer2idx = dataset.answer2idx\n",
        "        self.idx2question = dataset.idx2question\n",
        "        self.idx2answer = dataset.idx2answer\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(f\"{self.image_dir}/{self.df['image'][idx]}\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        question = process_text(self.df[\"question\"][idx])\n",
        "        question_tokenized = self.tokenizer(question, return_tensors=\"pt\", padding=True, truncation=True, max_length=30)\n",
        "\n",
        "        if self.answer:\n",
        "            answers = [self.answer2idx[process_text(answer[\"answer\"])] for answer in self.df[\"answers\"][idx]]\n",
        "            mode_answer_idx = mode(answers)  # 最頻値を取得（正解ラベル）\n",
        "            return image, question_tokenized, torch.Tensor(answers), int(mode_answer_idx)\n",
        "        else:\n",
        "            return image, question_tokenized\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "\n",
        "def VQA_criterion(batch_pred: torch.Tensor, batch_answers: torch.Tensor):\n",
        "    total_acc = 0.\n",
        "    for pred, answers in zip(batch_pred, batch_answers):\n",
        "        acc = 0.\n",
        "        for i in range(len(answers)):\n",
        "            num_match = 0\n",
        "            for j in range(len(answers)):\n",
        "                if i == j:\n",
        "                    continue\n",
        "                if pred == answers[j]:\n",
        "                    num_match += 1\n",
        "            acc += min(num_match / 3, 1)\n",
        "        total_acc += acc / 10\n",
        "    return total_acc / len(batch_pred)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BottleneckBlock(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels * self.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers):\n",
        "        super().__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self._make_layer(block, layers[0], 64)\n",
        "        self.layer2 = self._make_layer(block, layers[1], 128, stride=2)\n",
        "        self.layer3 = self._make_layer(block, layers[2], 256, stride=2)\n",
        "        self.layer4 = self._make_layer(block, layers[3], 512, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, 512)\n",
        "\n",
        "    def _make_layer(self, block, blocks, out_channels, stride=1):\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
        "\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(BottleneckBlock, [3, 4, 6, 3])\n",
        "\n",
        "\n",
        "class VQAModel(nn.Module):\n",
        "    def __init__(self, n_answer: int):\n",
        "        super().__init__()\n",
        "        self.resnet = ResNet18()\n",
        "        self.text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512 + 768, 512),  # 修正: 512（ResNetの出力）+ 768（BERTの出力）\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, n_answer)\n",
        "        )\n",
        "\n",
        "    def forward(self, image, question):\n",
        "        image_feature = self.resnet(image)  # 画像の特徴量 (batch_size, 512)\n",
        "        question_feature = self.text_encoder(**question).pooler_output  # テキストの特徴量 (batch_size, 768)\n",
        "\n",
        "        x = torch.cat([image_feature, question_feature], dim=1)  # 結合 (batch_size, 1280)\n",
        "        x = self.fc(x)  # 全結合層への入力\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    simple_acc = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for image, question, answers, mode_answer in dataloader:\n",
        "        image, question, answers, mode_answer = \\\n",
        "            image.to(device), {k: v.to(device) for k, v in question.items()}, answers.to(device), mode_answer.to(device)\n",
        "\n",
        "        pred = model(image, question)\n",
        "        loss = criterion(pred, mode_answer.squeeze())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
        "        simple_acc += (pred.argmax(1) == mode_answer).float().mean().item()  # simple accuracy\n",
        "\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
        "\n",
        "\n",
        "def eval(model, dataloader, optimizer, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    simple_acc = 0\n",
        "\n",
        "    start = time.time()\n",
        "    for image, question, answers, mode_answer in dataloader:\n",
        "        image, question, answers, mode_answer = \\\n",
        "            image.to(device), {k: v.to(device) for k, v in question.items()}, answers.to(device), mode_answer.to(device)\n",
        "\n",
        "        pred = model(image, question)\n",
        "        loss = criterion(pred, mode_answer.squeeze())\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_acc += VQA_criterion(pred.argmax(1), answers)  # VQA accuracy\n",
        "        simple_acc += (pred.argmax(1) == mode_answer).mean().item()  # simple accuracy\n",
        "\n",
        "    return total_loss / len(dataloader), total_acc / len(dataloader), simple_acc / len(dataloader), time.time() - start\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, questions, answers, mode_answers = zip(*batch)\n",
        "\n",
        "    # 画像をバッチにまとめる\n",
        "    images = torch.stack(images, dim=0)\n",
        "\n",
        "    # 質問文をバッチにまとめる\n",
        "    input_ids = [item['input_ids'].squeeze() for item in questions]\n",
        "    attention_mask = [item['attention_mask'].squeeze() for item in questions]\n",
        "\n",
        "    # パディング\n",
        "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    questions = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
        "\n",
        "    # 回答をバッチにまとめる\n",
        "    answers = torch.stack(answers, dim=0)\n",
        "\n",
        "    # 最頻値の回答をバッチにまとめる\n",
        "    mode_answers = torch.tensor(mode_answers)\n",
        "\n",
        "    return images, questions, answers, mode_answers\n",
        "\n",
        "def main():\n",
        "    # deviceの設定\n",
        "    set_seed(42)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    # tokenizerの設定\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # データの解凍\n",
        "    data_dir = \"/content/drive/MyDrive/dl-2024-final/data\"\n",
        "\n",
        "    # dataloader / model\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "    train_dataset = VQADataset(df_path=\"/content/drive/MyDrive/dl-2024-final/data/train.json\", image_dir=\"/content/drive/MyDrive/dl-2024-final/data/train\", transform=transform, tokenizer=tokenizer)\n",
        "    test_dataset = VQADataset(df_path=\"/content/drive/MyDrive/dl-2024-final/data/valid.json\", image_dir=\"/content/drive/MyDrive/dl-2024-final/data/valid\", transform=transform, tokenizer=tokenizer, answer=False)\n",
        "    test_dataset.update_dict(train_dataset)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    # 修正: vocab_sizeを削除し、n_answerのみを渡す\n",
        "    model = VQAModel(n_answer=len(train_dataset.answer2idx)).to(device)\n",
        "\n",
        "    # optimizer / criterion\n",
        "    num_epoch = 20\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "    # train model\n",
        "    for epoch in range(num_epoch):\n",
        "        train_loss, train_acc, train_simple_acc, train_time = train(model, train_loader, optimizer, criterion, device)\n",
        "        print(f\"【{epoch + 1}/{num_epoch}】\\n\"\n",
        "              f\"train time: {train_time:.2f} [s]\\n\"\n",
        "              f\"train loss: {train_loss:.4f}\\n\"\n",
        "              f\"train acc: {train_acc:.4f}\\n\"\n",
        "              f\"train simple acc: {train_simple_acc:.4f}\")\n",
        "\n",
        "    # 提出用ファイルの作成\n",
        "    model.eval()\n",
        "    submission = []\n",
        "    for image, question in test_loader:\n",
        "        image, question = image.to(device), {k: v.to(device) for k, v in question.items()}\n",
        "        pred = model(image, question)\n",
        "        pred = pred.argmax(1).cpu().item()\n",
        "        submission.append(pred)\n",
        "\n",
        "    submission = [train_dataset.idx2answer[id] for id in submission]\n",
        "    submission = np.array(submission)\n",
        "    torch.save(model.state_dict(), \"model.pth\")\n",
        "    np.save(\"submission.npy\", submission)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "xp2VOaHS_Rej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a41218-72a6-4583-c92c-3fd4579d9771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "【1/20】\n",
            "train time: 950.27 [s]\n",
            "train loss: 6.2718\n",
            "train acc: 0.4649\n",
            "train simple acc: 0.3744\n",
            "【2/20】\n",
            "train time: 915.53 [s]\n",
            "train loss: 5.3563\n",
            "train acc: 0.4724\n",
            "train simple acc: 0.3806\n",
            "【3/20】\n",
            "train time: 903.02 [s]\n",
            "train loss: 5.1567\n",
            "train acc: 0.4727\n",
            "train simple acc: 0.3812\n",
            "【4/20】\n",
            "train time: 905.68 [s]\n",
            "train loss: 5.0734\n",
            "train acc: 0.4725\n",
            "train simple acc: 0.3807\n",
            "【5/20】\n",
            "train time: 898.57 [s]\n",
            "train loss: 5.0398\n",
            "train acc: 0.4724\n",
            "train simple acc: 0.3807\n",
            "【6/20】\n",
            "train time: 891.16 [s]\n",
            "train loss: 4.9884\n",
            "train acc: 0.4715\n",
            "train simple acc: 0.3797\n"
          ]
        }
      ]
    }
  ]
}